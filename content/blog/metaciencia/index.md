---
title: "Crisis de replicabilidad y ciencia abierta en psicología"
subtitle: Los problemas y las soluciones de la ciencia psicológica
summary: Los problemas y las soluciones de la ciencia psicológica
authors: []
tags: [metaciencia]
categories: [metaciencia]
date: "2020-11-19"
lastMod: "2020-11-19"
featured: true
image:
  caption: ''
draft: 
links:
 - name: ""
   url: https://twitter.com/francosbenitez
   icon_pack: fab
   icon: twitter
---

---
Mucho ha sido discutido sobre crisis de replicabilidad y prácticas de ciencia abierta durante lo que va del siglo XXI. Desde 2005, un investigador se animaba a fundamentar que *la mayoría de los hallazgos de investigación publicados son falsos* (Ioannidis, 2005). Sucintamente, su crítica giraba en torno de las siguientes cuestiones: (a) tamaños de efecto pequeños, (b) sesgo de publicación, (c) manipulación de resultados y (d) poco poder estadístico (Charris, 2018).  

Lo cierto es que esas cuestiones también aplican a la psicología. En la última década han ocurrido algunos hitos que llamaron la atención sobre la validez de las investigaciones en este campo. Algunos de ellos son: (i) el fraude científico de Diederik Stapel, cuya manipulación de los datos significó un pobre funcionamiento de la crítica y el escrutinio científico debido a la ausencia de transparencia en la recolección de los mismos (Verfaellie & McGwin, 2011), (ii) los fallidos intentos de replicación de los estudios sobre percepción extrasensorial de Daryl Bem, quien reconoció haber usado sus datos como una herramienta de persuasión y nunca haberse preocupado acerca de si replicarían o no (Engber, 2017), (iii) los ocho estudios (de veintiuno de ellos) publicados en *Nature* y *Science* que no se pudieron replicar (Camerer et al., 2018), (iv) el *Reproducibility Project: Psychology* que pudo replicar sólo el 36% de los intentos y (v) los *Many Labs*: el *Many Labs* 1 replicó diez de trece; el *Many Labs* 2 replicó catorce de veintiocho; y, finalmente, el *Many Labs* 3 replicó sólo tres de los diez de ellos (Renkewitz & Heene, 2019; Stroebe, 2019; Yong, 2018). De este modo, llegó a ser claro que había un problema. *Inclusive, se consultó a 1576 científicos de diferentes ciencias si creían que había un problema de replicabilidad y el 90% respondió afirmativo* (Baker, 2016).

Algunas razones apuntadas como causantes de este problema fueron las siguientes: en primer lugar, el sesgo de publicación, es decir el hecho de que los estudios que no muestran efectos estadísticamente significativos o que reproducen el trabajo de otros no son publicados (Fanelli (2010) encontró que de 2434 *papers* publicados en psicología y psiquiatría el 91% son resultados positivos) (Chambers, 2017). En segundo lugar, las prácticas de investigación cuestionables (PICs), las cuales Anvari & Lakens (2018) definen como el conjunto de prácticas impulsadas por su utilidad en producir resultados estadísticos más favorables y que no son transparentemente reportadas en las secciones de métodos, y las que Morling & Calin-Jageman (2020) dividen en (1) *underreporting of null findings*, cuando los investigadores no reportan los efectos no significativos, (2) *p-hacking*, cuando los investigadores explotan los grados de libertad del investigador conducidos por el deseo de obtener un p-valor significante antes que por la meta de determinar cuán bien los datos apoyan las hipótesis, y, (3) *HARKing*, cuando los investigadores hipotetizan después de que los resultados son conocidos (*Hypothesizing After the Results are Known*). En tercer lugar, los incentivos perversos de las revistas y organismos de subvención, los cuales influenciados por la llamada cultura del “publica o perece” (*publish or perish*) premian el número de publicaciones científicas antes que la calidad de las mismas. En cuarto y último lugar, los malos usos y malentendidos estadísticos, siendo el p-valor el más frecuentemente malentendido, aunque también lo son los intervalos de confianza y el poder estadístico (por añadidura, Cassidy et al. (2019) examinaron treinta libros de psicología introductorios y encontraron que el 89% de los mismos definió la significación estadística incorrectamente) (Chambers, 2017; Greenland et al., 2016; Reenkewitz & Heene, 2019).

Sin embargo, no todo fue perplejidad y contemplación. La más grande iniciativa que se propuso para paliar esta situación fue el *movimiento por la ciencia abierta*. La ciencia abierta refiere al procedimiento de hacer el contenido y el proceso de producir afirmaciones y evidencia transparente y accesible para otros (Munafo et al., 2017, p.5). Así, el movimiento por la ciencia abierta considera la transparencia de la investigación como un método contra el error humano, el descuido, el sesgo de publicación y el fraude en la ciencia (Renkewitz & Heene, 2019, p.2).

Morling & Calin-Jageman (2020) dividen las nuevas prácticas que este movimiento promueve en dos ejes: 1) completa transparencia mediante materiales y datos abiertos y 2) reportes registrados y pre-registrados. El primer eje refiere a compartir i) cada variable dependiente y cada elección estadística (*full transparency*), ii) protocolos y estímulos experimentales completos (*open materials*) y iii) archivos de datos y libros de código de manera que otros investigadores puedan independientemente analizar los resultados (*open data*). El segundo eje alude, por el lado de la preregistración, al proceso de subir públicamente los procedimientos, hipótesis específicas y análisis de datos planeados *antes* de la recolección de datos, mientras que, por el lado de los reportes registrados, alude tanto al proceso de revisión por pares de la preregistración como a la aceptación condicional del estudio por parte de una revista sin importar el resultado final. En otras palabras, las nuevas prácticas que promueve el movimiento de la ciencia abierta intentan contrarrestar las causas y los efectos de la crisis de replicabilidad ya mencionados: el primer eje intenta prevenir el *underreporting of null findings* (no reportar los efectos no significativos) y el p-hacking (redondear el p-valor para lograr significación (Blincoe & Buchert, 2020)), mientras que el segundo intenta prevenir el HARKing (hipotetizar después de que los resultados son conocidos), el p-hacking y el sesgo de publicación (publicar sólo resultados positivos y novedosos).

Finalmente, como mencionan Mede et al. (2020), algunos investigadores distinguen entre replicabilidad [^1] y reproducibilidad: mientras que la primera alude a observar consistentemente ciertos resultados en *nuevas* muestras usando metodologías y condiciones *similares* a aquellas del estudio original, la reproducibilidad refiere a observar consistentemente ciertos resultados cuando se repiten los *mismos* procesamientos de datos y análisis estadísticos sobre los datos *originales* (LeBel et al., 2018, p.390; citado en Mede et al., 2020, p.9). Sin embargo, como también mencionan, otros autores tales como Chopik et al. (2018), quienes estudiaron sobre cómo enseñar acerca de la crisis de replicabilidad a estudiantes de psicología y cómo esto impacta en su confianza, y Anvari & Lakens (2018), quienes estudiaron sobre cómo impacta el conocimiento de la crisis de replicabilidad en la confianza del público, no hacen tal distinción y, antes bien, utilizan los términos intercambiablemente.

**Bibliografía**

Anvari, F., & Lakens, D. (2018). The replicability crisis and public trust in psychological science. Comprehensive Results in Social Psychology, 3(3), 266-286. https://doi.org/10.1080/23743603.2019.1684822

Baker, M. (2016). Is there a reproducibility crisis? Nature, 452–454(26). doi: 10.1038/533452a

Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T.-H., Huber, J., Johannesson, M., . . . Pfeiffer, T. (2018). Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015. Nature Human Behaviour, 2, 637–644. https://doi.org/10.1038/s41562-018-0399-z

Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., & Stanley, D. J. (2019). Failing grade: 89% of introduction-to-psychology textbooks that define or explain statistical significance do so incorrectly. Advances in Methods and Practices in Psychological Science, 2(3), 233-239.

Chambers, C., (2017). The seven deadly sins of psychology: A manifesto for reforming the culture of scientific practice, Princeton, New Jersey: Princeton University Press.

Charris Domínguez, R. A. (2018). Cómo evaluar una réplica en psicología (tesis de máster). Universidad de los Andes, Bogotá, Colombia.

Chopik, W. J., Bremner, R. H., Defever, A. M., & Keller, V. N. (2018). How (and whether) to teach undergraduates about the replication crisis in psychological science. Teaching of psychology, 45(2), 158-163. https://doi.org/10.1177/0098628318762900

Engber, D. (2017, 17 de mayo). Daryl Bem proved ESP is real: Which means science is broken. Slate. Recuperado de https://slate.com/health-and-science/2017/06/daryl-bem-proved-esp-is-real-showed-science-is-broken.html

Fanelli, D. (2010). “Positive” Results Increase Down the Hierarchy of the Sciences. PLOS ONE, 5(4), 1-10. https://doi.org/10.1371/journal.pone.0010068

Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., & Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. European journal of epidemiology, 31(4), 337-350. https://doi.org/10.1007/s10654-016-0149-3

Ioannidis, J. P. (2005). Why most published research findings are false. PLoS medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124

Morling, B., & Calin-Jageman, R. J. (2020). What Psychology Teachers Should Know About Open Science and the New Statistics. Teaching of Psychology, 47(2), 169-179. https://doi.org/10.1177/0098628320901372

Mede, N. G., Schäfer, M. S., Ziegler, R., & Weißkopf, M. (2020). The “replication crisis” in the public eye: Germans’ awareness and perceptions of the (ir)reproducibility of scientific research. Public Understanding of Science, 12.

Munafò, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C. D., Du Sert, N. P., ... & Ioannidis, J. P. (2017). A manifesto for reproducible science. Nature human behaviour, 1(1), 1-9. https://doi.org/10.1038/s41562-016-0021

Renkewitz, F., & Heene, M. (2019). The Replication Crisis and Open Science in Psychology: Methodological Challenges and Developments. Zeitschrift Für Psychologie, 227(4), 233–236. https://doi.org/10.1027/2151-2604/a000389

Stroebe, W. (2019). What Can We Learn from Many Labs Replications? Basic and Applied Social Psychology, 41:2, 91-103. https://doi.org/10.1080/01973533.2019.1577736

Verfaellie, M., & McGwin, J. (2011, diciembre). The case of Diederik Stepel. Psychological Science Agenda. Recuperado de
https://www.apa.org/science/about/psa/2011/12/diederik-stapel

Yong, E. (2018, 19 de noviembre). Psychology’s Replication Crisis Is Running Out of Excuses. The Atlantic. Recuperado de
https://www.theatlantic.com/science/archive/2018/11/psychologys-replication-crisis-real/576223/ 

[^1]: Charris (2018) diferencia entre la replicación directa y la conceptual: mientras que la primera refiere a intentar reproducir el estudio original utilizando los mismos métodos, materiales y forma de presentación teniendo como única diferencia la muestra, la segunda alude a poner a poner a prueba la misma hipótesis del estudio original con otros diseños. Según este autor, la interpretación de ambas réplicas se debe interpretar de manera distinta, puesto que en las réplicas directas existe mayor control. Las réplicas directas son las que se han llevado a cabo en proyectos ya mencionados tales como el *Reproducibility Project: Psychology* y los *Many Labs*.